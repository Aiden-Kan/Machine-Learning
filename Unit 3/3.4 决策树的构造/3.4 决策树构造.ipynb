{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-spine",
   "metadata": {},
   "source": [
    "## 任务描述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-fraud",
   "metadata": {},
   "source": [
    "1. 了解决策树基本概念\n",
    "\n",
    "2. 掌握构造决策树的常见算法\n",
    "\n",
    "3. 了解决策树的剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-prefix",
   "metadata": {},
   "source": [
    "## 相关知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-finger",
   "metadata": {},
   "source": [
    "决策树（decision tree）：是一种基本的分类与回归方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-modem",
   "metadata": {},
   "source": [
    "决策树通常有三个步骤：特征选择、决策树的生成、决策树的修剪。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-emperor",
   "metadata": {},
   "source": [
    "决策树的构建是一种自上而下的归纳过程，用样本的属性作为节点，属性的取值作为分值的树形结构。因此，每棵决策树对应着从根节点到叶子节点的一组规则。\n",
    "![jupyter](https://i.loli.net/2021/06/01/KzJgZ61rH7SdmMX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-stationery",
   "metadata": {},
   "source": [
    "**决策树的构造过程**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-hello",
   "metadata": {},
   "source": [
    "1） 开始：构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-disabled",
   "metadata": {},
   "source": [
    "2） 如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-setup",
   "metadata": {},
   "source": [
    "3）如果还有子集不能够被正确的分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如果递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-selling",
   "metadata": {},
   "source": [
    "4）每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-reminder",
   "metadata": {},
   "source": [
    "**特征选择** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-chess",
   "metadata": {},
   "source": [
    "划分数据集的原则是：将无序数据变得更加有序，但是各种方法都有各自的优缺点，信息论是量化处理信息的分支科学，在划分数据集前后信息发生的变化称为信息增益，获得信息增益最高的特征就是最好的选择。特征选择就是决定用哪个特征来划分特征空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-yahoo",
   "metadata": {},
   "source": [
    "**信息增益**：信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：\n",
    "![jupyter](https://i.loli.net/2021/06/01/BLNiWYs3wcMSUJk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-worse",
   "metadata": {},
   "source": [
    "**信息增益比**：特征A对训练数据集D的信息增益比g_R(D,A)定义为其信息增益 g(D,A)与训练数据集D的经验熵之比：\n",
    "![jupyter](https://i.loli.net/2021/06/01/4UPtTYR7hCialKS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-howard",
   "metadata": {},
   "source": [
    "**决策树构建**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-cooking",
   "metadata": {},
   "source": [
    "构建决策树的算法有很多，比如ID3算法、C4.5算法、CLS算法和CART算法等等。在本节内容，我们主要讲解ID3算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-calvin",
   "metadata": {},
   "source": [
    "**ID3算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-guidance",
   "metadata": {},
   "source": [
    "ID3算法是决策树算法的代表，绝大多数决策树算法都是在它的基础上加以改进而实现的。它采用了分治策略。核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-saying",
   "metadata": {},
   "source": [
    "具体方法：检测所有的属性，选择信息增益最大的属性产生决策树节点，由该属性的不同取值建立分支，再对各分支的子集递归调用该方法建立决策树节点分支，直到所有子集仅包含同一类别的数据为止。最后得到一棵决策树，它可以对新的样本进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-sender",
   "metadata": {},
   "source": [
    "算法的过程为：\n",
    "\n",
    "　　1) 初始化信息增益的阈值ϵ\n",
    "\n",
    "　　2）判断样本是否为同一类输出Di，如果是则返回单节点树T。标记类别为Di\n",
    "\n",
    "　　3) 判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。\n",
    "\n",
    "　　4）计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征Ag\n",
    "\n",
    "　　5) 如果Ag的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。\n",
    "\n",
    "　　6）否则，按特征Ag的不同取值Agi将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为Agi。返回增加了节点的数T。\n",
    "\n",
    "　　7）对于所有的子节点，令D=Di,A=A−{Ag}递归调用2-6步，得到子树Ti并返回。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-canvas",
   "metadata": {},
   "source": [
    "**C4.5的生成算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-dayton",
   "metadata": {},
   "source": [
    "与ID3算法相似，但是做了改进，将**信息增益比**作为选择特征的标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-cutting",
   "metadata": {},
   "source": [
    "接下来我们来实现一下ID3算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-crossing",
   "metadata": {},
   "source": [
    "![jupyter](https://i.loli.net/2021/06/01/o4SfB3gVwrt8HGT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-projector",
   "metadata": {},
   "source": [
    "在编写代码之前，我们先对数据集进行属性标注。\n",
    "\n",
    "年龄：0代表青年，1代表中年，2代表老年；\n",
    "\n",
    "有工作：0代表否，1代表是；\n",
    "\n",
    "有自己的房子：0代表否，1代表是；\n",
    "\n",
    "信贷情况：0代表一般，1代表好，2代表非常好；\n",
    "\n",
    "类别(是否给贷款)：no代表否，yes代表是。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "average-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-method",
   "metadata": {},
   "source": [
    "**创建数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recognized-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    # 数据集\n",
    "    dataSet=[[0, 0, 0, 0, 'no'],\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    #分类属性\n",
    "    labels=['年龄','有工作','有自己的房子','信贷情况']\n",
    "    #返回数据集和分类属性\n",
    "    return dataSet,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-disabled",
   "metadata": {},
   "source": [
    "**计算给定数据集的经验熵**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "specialized-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcShannonEnt(dataSet):\n",
    "    #返回数据集行数\n",
    "    numEntries=len(dataSet)\n",
    "    \n",
    "    #保存每个标签（label）出现次数的字典\n",
    "    labelCounts={}\n",
    "    #对每组特征向量进行统计\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1]                     #提取标签信息\n",
    "        if currentLabel not in labelCounts.keys():   #如果标签没有放入统计次数的字典，添加进去\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1                 #label计数\n",
    "    \n",
    "    shannonEnt=0.0                                   #经验熵\n",
    "    #计算经验熵\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries      #选择该标签的概率\n",
    "        shannonEnt-=prob*log(prob,2)                 #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-animal",
   "metadata": {},
   "source": [
    "**按照给定特征划分数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interior-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet,axis,value):\n",
    "    #创建数据集列表\n",
    "    retDataSet=[]\n",
    "    #遍历数据集\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            #去掉axis特征\n",
    "            reduceFeatVec=featVec[:axis]\n",
    "            #将符合条件的添加到返回的数据集\n",
    "            reduceFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reduceFeatVec)\n",
    "    #返回划分后的数据集\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-cornwall",
   "metadata": {},
   "source": [
    "**获取信息增益最大特征的索引值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "negative-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    #特征数量\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    #计数数据集的香农熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    #信息增益\n",
    "    bestInfoGain = 0.0\n",
    "    #最优特征的索引值\n",
    "    bestFeature = -1\n",
    "    #遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        #创建set集合{}，元素不可重复\n",
    "        uniqueVals = set(featList)\n",
    "        #经验条件熵\n",
    "        newEntropy = 0.0\n",
    "        #计算信息增益\n",
    "        for value in uniqueVals:\n",
    "            #subDataSet划分后的子集\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            #计算子集的概率\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            #根据公式计算经验条件熵\n",
    "            newEntropy += prob * calcShannonEnt((subDataSet))\n",
    "        #信息增益\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        #打印每个特征的信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))\n",
    "        #计算信息增益\n",
    "        if (infoGain > bestInfoGain):\n",
    "            #更新信息增益，找到最大的信息增益\n",
    "            bestInfoGain = infoGain\n",
    "            #记录信息增益最大的特征的索引值\n",
    "            bestFeature = i\n",
    "            #返回信息增益最大特征的索引值\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-brook",
   "metadata": {},
   "source": [
    "**统计classList中出现次数最多的元素（类标签）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conservative-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    #统计classList中每个元素出现的次数\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "            classCount[vote]+=1\n",
    "        #根据字典的值降序排列\n",
    "        sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-projector",
   "metadata": {},
   "source": [
    "**创建决策树**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "finished-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet,labels,featLabels):\n",
    "    #取分类标签（是否放贷：yes or no）\n",
    "    classList=[example[-1] for example in dataSet]\n",
    "    #如果类别完全相同，则停止继续划分\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    #遍历完所有特征时返回出现次数最多的类标签\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    #选择最优特征\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet)\n",
    "    #最优特征的标签\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    #根据最优特征的标签生成树\n",
    "    myTree={bestFeatLabel:{}}\n",
    "    #删除已经使用的特征标签\n",
    "    del(labels[bestFeat])\n",
    "    #得到训练集中所有最优特征的属性值\n",
    "    featValues=[example[bestFeat] for example in dataSet]\n",
    "    #去掉重复的属性值\n",
    "    uniqueVls=set(featValues)\n",
    "    #遍历特征，创建决策树\n",
    "    for value in uniqueVls:\n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),\n",
    "                                               labels,featLabels)\n",
    "    return myTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occasional-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "第0个特征的增益为0.252\n",
      "第1个特征的增益为0.918\n",
      "第2个特征的增益为0.474\n",
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    dataSet,labels=createDataSet()\n",
    "    featLabels=[]\n",
    "    myTree=createTree(dataSet,labels,featLabels)\n",
    "    print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-worse",
   "metadata": {},
   "source": [
    "**决策树的过拟合**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-sperm",
   "metadata": {},
   "source": [
    "当决策树深度过大时，在训练集上表现特别好，往往就会出现过拟合现象，我们需要一些解决办法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-oriental",
   "metadata": {},
   "source": [
    "**剪枝**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-shelter",
   "metadata": {},
   "source": [
    "总体思路：\n",
    "\n",
    "   由完全树T0开始，剪枝部分结点得到树T1，然后再剪枝部分结点得到树T2，…，直到仅剩树根的Tk。在验证数据集上对这K个树分别评价，选择损失函数最小的树Tm。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-pressure",
   "metadata": {},
   "source": [
    "## 编程要求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-glucose",
   "metadata": {},
   "source": [
    "复习上述内容，完成创建决策树createTree()函数编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator\n",
    "\n",
    "\"\"\"\n",
    "函数说明：计算给定数据集的经验熵（香农熵）\n",
    "Parameters：\n",
    "    dataSet：数据集\n",
    "Returns：\n",
    "    shannonEnt：经验熵\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    #返回数据集行数\n",
    "    numEntries=len(dataSet)\n",
    "    #保存每个标签（label）出现次数的字典\n",
    "    labelCounts={}\n",
    "    #对每组特征向量进行统计\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1]                     #提取标签信息\n",
    "        if currentLabel not in labelCounts.keys():   #如果标签没有放入统计次数的字典，添加进去\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1                 #label计数\n",
    "\n",
    "    shannonEnt=0.0                                   #经验熵\n",
    "    #计算经验熵\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries      #选择该标签的概率\n",
    "        shannonEnt-=prob*log(prob,2)                 #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵\n",
    "\n",
    "\"\"\"\n",
    "函数说明：创建测试数据集\n",
    "Parameters：无\n",
    "Returns：\n",
    "    dataSet：数据集\n",
    "    labels：分类属性\n",
    "\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    # 数据集\n",
    "    dataSet=[[0, 0, 0, 0, 'no'],\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    #分类属性\n",
    "    labels=['年龄','有工作','有自己的房子','信贷情况']\n",
    "    #返回数据集和分类属性\n",
    "    return dataSet,labels\n",
    "\n",
    "\"\"\"\n",
    "函数说明：按照给定特征划分数据集\n",
    "\n",
    "Parameters：\n",
    "    dataSet:待划分的数据集\n",
    "    axis：划分数据集的特征\n",
    "    value：需要返回的特征值\n",
    "\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    #创建返回的数据集列表\n",
    "    retDataSet=[]\n",
    "    #遍历数据集\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            #去掉axis特征\n",
    "            reduceFeatVec=featVec[:axis]\n",
    "            #将符合条件的添加到返回的数据集\n",
    "            reduceFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reduceFeatVec)\n",
    "    #返回划分后的数据集\n",
    "    return retDataSet\n",
    "\n",
    "\"\"\"\n",
    "函数说明：计算给定数据集的经验熵（香农熵）\n",
    "Parameters：\n",
    "    dataSet：数据集\n",
    "Returns：\n",
    "    shannonEnt：信息增益最大特征的索引值\n",
    "\"\"\"\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    #特征数量\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    #计数数据集的香农熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    #信息增益\n",
    "    bestInfoGain = 0.0\n",
    "    #最优特征的索引值\n",
    "    bestFeature = -1\n",
    "    #遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        #创建set集合{}，元素不可重复\n",
    "        uniqueVals = set(featList)\n",
    "        #经验条件熵\n",
    "        newEntropy = 0.0\n",
    "        #计算信息增益\n",
    "        for value in uniqueVals:\n",
    "            #subDataSet划分后的子集\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            #计算子集的概率\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            #根据公式计算经验条件熵\n",
    "            newEntropy += prob * calcShannonEnt((subDataSet))\n",
    "        #信息增益\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        #打印每个特征的信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))\n",
    "        #计算信息增益\n",
    "        if (infoGain > bestInfoGain):\n",
    "            #更新信息增益，找到最大的信息增益\n",
    "            bestInfoGain = infoGain\n",
    "            #记录信息增益最大的特征的索引值\n",
    "            bestFeature = i\n",
    "            #返回信息增益最大特征的索引值\n",
    "    return bestFeature\n",
    "\n",
    "\"\"\"\n",
    "函数说明：统计classList中出现次数最多的元素（类标签）\n",
    "Parameters：\n",
    "    classList：类标签列表\n",
    "Returns：\n",
    "    sortedClassCount[0][0]：出现次数最多的元素（类标签）\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    #统计classList中每个元素出现的次数\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "            classCount[vote]+=1\n",
    "        #根据字典的值降序排列\n",
    "        sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "        return sortedClassCount[0][0]\n",
    "\n",
    "\"\"\"\n",
    "函数说明：创建决策树\n",
    "\n",
    "Parameters:\n",
    "    dataSet：训练数据集\n",
    "    labels：分类属性标签\n",
    "    featLabels：存储选择的最优特征标签\n",
    "Returns：\n",
    "    myTree：决策树\n",
    "\"\"\"\n",
    "def createTree(dataSet,labels,featLabels):\n",
    "************** Begin **************\n",
    "\n",
    "\n",
    "\n",
    "**************  End  **************\n",
    "\n",
    "if __name__=='__main__':\n",
    "    dataSet,labels=createDataSet()\n",
    "    featLabels=[]\n",
    "    myTree=createTree(dataSet,labels,featLabels)\n",
    "    print(myTree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-munich",
   "metadata": {},
   "source": [
    "## 参考答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取分类标签（是否放贷：yes or no）\n",
    "    classList=[example[-1] for example in dataSet]\n",
    "    #如果类别完全相同，则停止继续划分\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    #遍历完所有特征时返回出现次数最多的类标签\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    #选择最优特征\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet)\n",
    "    #最优特征的标签\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    #根据最优特征的标签生成树\n",
    "    myTree={bestFeatLabel:{}}\n",
    "    #删除已经使用的特征标签\n",
    "    del(labels[bestFeat])\n",
    "    #得到训练集中所有最优特征的属性值\n",
    "    featValues=[example[bestFeat] for example in dataSet]\n",
    "    #去掉重复的属性值\n",
    "    uniqueVls=set(featValues)\n",
    "    #遍历特征，创建决策树\n",
    "    for value in uniqueVls:\n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),\n",
    "                                               labels,featLabels)\n",
    "    return myTree\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
