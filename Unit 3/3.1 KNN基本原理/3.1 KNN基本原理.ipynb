{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hollywood-office",
   "metadata": {},
   "source": [
    "## 任务描述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-andrews",
   "metadata": {},
   "source": [
    "1. 掌握KNN算法基本原理\n",
    "\n",
    "2. 掌握KNN算法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-effect",
   "metadata": {},
   "source": [
    "## 相关知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-princeton",
   "metadata": {},
   "source": [
    "**KNN基本原理**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-exception",
   "metadata": {},
   "source": [
    "   KNN（K-Nearest Neighbor）最邻近分类算法是数据挖掘分类（classification）技术中最简单的算法之一，其指导思想是“近朱者赤，近墨者黑”，即由你的邻居来推断出你的类别。\n",
    "   \n",
    "   K个最近邻居，毫无疑问，K的取值肯定是至关重要的。那么最近的邻居又是怎么回事呢？其实，KNN的原理就是当预测一个新的值x的时候，根据它距离最近的K个点是什么类别来判断x属于哪个类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-dress",
   "metadata": {},
   "source": [
    "![jupyter](https://i.loli.net/2021/05/30/XfrUHoEkjOuGlcK.png)\n",
    "![jupyter](https://i.loli.net/2021/05/30/XVwayNnxBGd2omg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-color",
   "metadata": {},
   "source": [
    "图中绿色的点就是我们要预测的那个点，假设K=3。那么KNN算法就会找到与它距离最近的三个点（这里用圆圈把它圈起来了），看看哪种类别多一些，比如这个例子中是蓝色三角形多一些，新来的绿色点就归类到蓝三角了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-terror",
   "metadata": {},
   "source": [
    "![jupyter](https://i.loli.net/2021/05/30/bFntI9M8mBACOGr.png)\n",
    "![jupyter](https://i.loli.net/2021/05/30/uejIkQpCK4GVmLr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-transmission",
   "metadata": {},
   "source": [
    "当K=5的时候，判定就变成不一样了。这次变成红圆多一些，所以新来的绿点被归类成红圆。从这个例子中，我们就能看得出K的取值是很重要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-prerequisite",
   "metadata": {},
   "source": [
    "**KNN最邻近分类算法的实现原理**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-court",
   "metadata": {},
   "source": [
    "为了判断未知样本的类别，以所有已知类别的样本作为参照，计算未知样本与所有已知样本的距离，从中选取与未知样本距离最近的K个已知样本，根据少数服从多数的投票法则（majority-voting），将未知样本与K个最邻近样本中所属类别占比较多的归为一类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-replication",
   "metadata": {},
   "source": [
    "**KNN算法的关键**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-victim",
   "metadata": {},
   "source": [
    "(1) 样本的所有特征都要做可比较的量化\n",
    "\n",
    "   若是样本特征中存在非数值的类型，必须采取手段将其量化为数值。例如样本特征中包含颜色，可通过将颜色转换为灰度值来实现距离计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-holly",
   "metadata": {},
   "source": [
    "(2) 样本特征要做归一化处理\n",
    "\n",
    "   样本有多个参数，每一个参数都有自己的定义域和取值范围，他们对距离计算的影响不一样，如取值较大的影响力会盖过取值较小的参数。所以样本参数必须做一些 scale 处理，最简单的方式就是所有特征的数值都采取归一化处置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-heaven",
   "metadata": {},
   "source": [
    "(3) 需要一个距离函数以计算两个样本之间的距离\n",
    "\n",
    "   通常使用的距离函数有：欧氏距离、余弦距离、汉明距离、曼哈顿距离等，一般选欧氏距离作为距离度量，但是这是只适用于连续变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-islam",
   "metadata": {},
   "source": [
    "![jupyter](https://i.loli.net/2021/05/30/dAvCuewZGEBqxtL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-singer",
   "metadata": {},
   "source": [
    "欧式距离"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-insulin",
   "metadata": {},
   "source": [
    "![jupyrer](https://i.loli.net/2021/05/30/V39SYIODfb2Xgdu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-hydrogen",
   "metadata": {},
   "source": [
    "拓展到多维空间，则变为了\n",
    "![jupyrer](https://i.loli.net/2021/05/30/qNg8pFW3M5HteQB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-lover",
   "metadata": {},
   "source": [
    "(4) 确定K的值\n",
    "\n",
    "   K值选的太大易引起欠拟合，太小容易过拟合，需交叉验证确定K值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-jumping",
   "metadata": {},
   "source": [
    "**KNN算法的优点**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-japan",
   "metadata": {},
   "source": [
    "1.简单，易于理解，易于实现，无需估计参数，无需训练；\n",
    "\n",
    "2.适合对稀有事件进行分类；\n",
    "\n",
    "3.特别适合于多分类问题(multi-modal,对象具有多个类别标签)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-transparency",
   "metadata": {},
   "source": [
    "**KNN算法的缺点**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-approval",
   "metadata": {},
   "source": [
    "1.对不相关的功能和数据规模敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-conservative",
   "metadata": {},
   "source": [
    "## 编程要求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-greeting",
   "metadata": {},
   "source": [
    "复习上述内容，在Begin和End之间编写calEuclideanDistance()函数，计算v1和v2之间的欧氏距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def calEuclideanDistance(vec1,vec2):\n",
    "************ Begin ************\n",
    "\n",
    "\n",
    "************  End  ************\n",
    "v1 = [1,2]\n",
    "v2 = [2,3]\n",
    "v1 = numpy.array(v1)\n",
    "v2 = numpy.array(v2)\n",
    "print(calEuclideanDistance(v1,v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-bridal",
   "metadata": {},
   "source": [
    "## 参考答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = numpy.sqrt(numpy.sum(numpy.square(vec1 - vec2)))\n",
    "return dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
